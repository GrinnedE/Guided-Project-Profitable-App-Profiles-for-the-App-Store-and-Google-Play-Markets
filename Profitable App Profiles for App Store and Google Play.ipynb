{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbe0a8ee",
   "metadata": {},
   "source": [
    "# Profitable App Profiles for the App Store and Google Play Markets\n",
    "\n",
    "We try to find a profitable free app profile that will suit both the App Store and Google Play markets. For that task, we will be using two data sets - [one](https://www.kaggle.com/ramamet4/app-store-apple-data-set-10k-apps) containing data about around 7200 App Store applications, and the [second](https://www.kaggle.com/lava18/google-play-store-apps) one containing data about around 10 000 Google Play applications. \n",
    "\n",
    "My goal in this project is to further my data science skills with some hands-on problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7199e8",
   "metadata": {},
   "source": [
    "First, let's start by opening our data sets, loading them into lists and then printing the first few rows of each data set to make sure they've been loaded correctly. Let's also count the amount of columns and rows to make sure that we've loaded all data into our lists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4538e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explore_data(dataset, start, end, rows_and_columns=False):\n",
    "    dataset_slice = dataset[start:end]    \n",
    "    for row in dataset_slice:\n",
    "        print(row, '\\n')\n",
    "\n",
    "    if rows_and_columns:\n",
    "        print('Number of rows:', len(dataset))\n",
    "        print('Number of columns:', len(dataset[0]))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37dd26bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(dataset_path):\n",
    "    \n",
    "    from csv import reader\n",
    "\n",
    "    open_dataset = open(dataset_path, encoding='utf8')\n",
    "    read_dataset = reader(open_dataset)\n",
    "    list_dataset = list(read_dataset)    \n",
    "    return list_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f0dcb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'id', 'track_name', 'size_bytes', 'currency', 'price', 'rating_count_tot', 'rating_count_ver', 'user_rating', 'user_rating_ver', 'ver', 'cont_rating', 'prime_genre', 'sup_devices.num', 'ipadSc_urls.num', 'lang.num', 'vpp_lic'] \n",
      "\n",
      "['1', '281656475', 'PAC-MAN Premium', '100788224', 'USD', '3.99', '21292', '26', '4', '4.5', '6.3.5', '4+', 'Games', '38', '5', '10', '1'] \n",
      "\n",
      "['2', '281796108', 'Evernote - stay organized', '158578688', 'USD', '0', '161065', '26', '4', '3.5', '8.2.2', '4+', 'Productivity', '37', '5', '23', '1'] \n",
      "\n",
      "['3', '281940292', 'WeatherBug - Local Weather, Radar, Maps, Alerts', '100524032', 'USD', '0', '188583', '2822', '3.5', '4.5', '5.0.0', '4+', 'Weather', '37', '5', '3', '1'] \n",
      "\n",
      "['4', '282614216', 'eBay: Best App to Buy, Sell, Save! Online Shopping', '128512000', 'USD', '0', '262241', '649', '4', '4.5', '5.10.0', '12+', 'Shopping', '37', '5', '9', '1'] \n",
      "\n",
      "Number of rows: 7198\n",
      "Number of columns: 17\n",
      "\n",
      "\n",
      "['App', 'Category', 'Rating', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver', 'Android Ver'] \n",
      "\n",
      "['Photo Editor & Candy Camera & Grid & ScrapBook', 'ART_AND_DESIGN', '4.1', '159', '19M', '10,000+', 'Free', '0', 'Everyone', 'Art & Design', 'January 7, 2018', '1.0.0', '4.0.3 and up'] \n",
      "\n",
      "['Coloring book moana', 'ART_AND_DESIGN', '3.9', '967', '14M', '500,000+', 'Free', '0', 'Everyone', 'Art & Design;Pretend Play', 'January 15, 2018', '2.0.0', '4.0.3 and up'] \n",
      "\n",
      "['U Launcher Lite â€“ FREE Live Cool Themes, Hide Apps', 'ART_AND_DESIGN', '4.7', '87510', '8.7M', '5,000,000+', 'Free', '0', 'Everyone', 'Art & Design', 'August 1, 2018', '1.2.4', '4.0.3 and up'] \n",
      "\n",
      "['Sketch - Draw & Paint', 'ART_AND_DESIGN', '4.5', '215644', '25M', '50,000,000+', 'Free', '0', 'Teen', 'Art & Design', 'June 8, 2018', 'Varies with device', '4.2 and up'] \n",
      "\n",
      "Number of rows: 10842\n",
      "Number of columns: 13\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appstore_dataset = read_data('E:\\Jupyter\\Profitable App Profiles for App Store and Google Play\\AppleStore.csv')\n",
    "googleplay_dataset = read_data('E:\\Jupyter\\Profitable App Profiles for App Store and Google Play\\googleplaystore.csv')\n",
    "\n",
    "explore_data(appstore_dataset, 0, 5, True)\n",
    "explore_data(googleplay_dataset, 0, 5, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585b3498",
   "metadata": {},
   "source": [
    "As we can see above, our lists seem to be loaded correctly. Now, let's assume that our app has to be free and directed towards english-speaking audience. We need to clean our data sets by removing wrong data and paid or non-english apps and then print column names(descriptions of which can be found in the links provided at the beginning) to find columns that might help us with our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1036df87",
   "metadata": {},
   "source": [
    "The Google Play data set has a dedicated discussion section, where one of [discussions](https://www.kaggle.com/lava18/google-play-store-apps/discussion/66015) mentions an error at entry 10472(10473 with header included). Let's print that row and see if it really is incorrect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d81fe6cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Life Made WI-Fi Touchscreen Photo Frame', '1.9', '19', '3.0M', '1,000+', 'Free', '0', 'Everyone', '', 'February 11, 2018', '1.0.19', '4.0 and up']\n"
     ]
    }
   ],
   "source": [
    "print(googleplay_dataset[10473])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef7d9d5",
   "metadata": {},
   "source": [
    "It is indeed incorrect. What we can do now is just simply remove that row with `del` statement. Let's print that row again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da07cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['osmino Wi-Fi: free WiFi', 'TOOLS', '4.2', '134203', '4.1M', '10,000,000+', 'Free', '0', 'Everyone', 'Tools', 'August 7, 2018', '6.06.14', '4.4 and up']\n"
     ]
    }
   ],
   "source": [
    "del googleplay_dataset[10473]\n",
    "print(googleplay_dataset[10473])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3f896",
   "metadata": {},
   "source": [
    "Now that we know there are no more errors in our data, we should check our data sets for duplicates by writing a simple function that will check apps' names and create a list of unique apps and a list of duplicates. Note that our data sets are built in a different order, so we need to add `name_index` argument to our function, which will let us work with both data sets using only one function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d4d494c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_duplicates(dataset, name_index):\n",
    "    duplicate_apps = []\n",
    "    unique_apps = []\n",
    "    for row in dataset[1:]:\n",
    "        if row[0] in unique_apps:\n",
    "            duplicate_apps.append(row[name_index])\n",
    "        else:\n",
    "            unique_apps.append(row[name_index])\n",
    "    print('There are ' + str(len(duplicate_apps)) + ' duplicate apps in the data set')\n",
    "    len(duplicate_apps)\n",
    "    return len(duplicate_apps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6df8e1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1181 duplicate apps in the data set\n",
      "There are 0 duplicate apps in the data set\n"
     ]
    }
   ],
   "source": [
    "android_duplicates = find_duplicates(googleplay_dataset, 0)\n",
    "appstore_duplicates = find_duplicates(appstore_dataset, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85325e0",
   "metadata": {},
   "source": [
    "We can see there are quite a few duplicates in our data set that we will need to remove. Since we can't do that randomly, we have to find a criterion which will make our data the most accurate. In this case, we can judge the relevance of our data by the amount of user reviews(the latest app data has the most reviews). \n",
    "\n",
    "To check if our function works correctly, we can calculate the expected amount of unique apps and compare it with the length of our list after removing duplicates from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9b53bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_android(dataset):\n",
    "    reviews_max = {}\n",
    "    for row in dataset[1:]:\n",
    "        if row[0] in reviews_max:\n",
    "            if row[3] > reviews_max[row[0]]:\n",
    "                reviews_max[row[0]] = row[3]\n",
    "        else:\n",
    "            reviews_max[row[0]] = row[3]\n",
    "    print('Expected number of unique android apps: ', len(dataset) - 1 -android_duplicates)\n",
    "    print('Our number of unique android apps: ', len(reviews_max))\n",
    "    clean_data = []\n",
    "    clean_data.append(dataset[0])\n",
    "    added_apps = []\n",
    "    for row in dataset[1:]:\n",
    "        if row[3] == reviews_max[row[0]] and row[0] not in added_apps:\n",
    "            clean_data.append(row)\n",
    "            added_apps.append(row[0])\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fda61",
   "metadata": {},
   "source": [
    "In the code above, we created a dictionary to hold the unique names of our apps and their highest amount of reviews. It allowed us later to create two empty lists - for cleaned data and for the names of apps that have already been put into that list. \n",
    "\n",
    "Then, we simply just looped through our data set(excluding header) and checked if the current app's number of reviews equals its highest amount possible saved in the `reviews_max` and if its name has already been added to our control list called `added_apps` that ensures no duplicates are put into `clean_data` list. \n",
    "\n",
    "To make sure that our function work correctly, we also checked if the length of cleaned list equals the expected number of unique apps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "319882fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of unique android apps:  9659\n",
      "Our number of unique android apps:  9659\n"
     ]
    }
   ],
   "source": [
    "googleplay_dataset = remove_duplicates_android(googleplay_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1f523",
   "metadata": {},
   "source": [
    "We can finally move on to the next problem - removing non-english apps. To do that, we should know that each character we use has its own corresponding number, which we can get by using `ord()` function. According to ASCII system, the characters commonly used in english texts are numbered 0 to 127. Since some english apps might use non-english characters, we will assume that non-english app is an app that contains more than 3 non-english characters in its name. We can clean non-english apps from our data sets by checking each row's name column for characters with assigned number outside of our 0-127 range. To do that, we will create a function that iterates through our lists and then removes every app that contains more than 3 non-english characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eba81ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english(dataset, name_index):\n",
    "    clean_data = []\n",
    "    for row in dataset:\n",
    "        non_english_chars = 0\n",
    "        for character in row[name_index]:\n",
    "            if ord(character) > 127:\n",
    "                non_english_chars += 1\n",
    "        if non_english_chars <= 3:\n",
    "            clean_data.append(row)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e462a7",
   "metadata": {},
   "source": [
    "Let's create some simple and short test lists to check if our functions work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe784480",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Instagram'], ['Docs To Goâ„¢ Free Office Suite'], ['Instachat ðŸ˜œ']] \n",
      " [['', '', 'Instagram'], ['', '', 'Docs To Goâ„¢ Free Office Suite'], ['', '', 'Instachat ðŸ˜œ']]\n"
     ]
    }
   ],
   "source": [
    "test_list_android = [['Instagram'], ['çˆ±å¥‡è‰ºPPS -ã€Šæ¬¢ä¹é¢‚2ã€‹ç”µè§†å‰§çƒ­æ’­'], ['Docs To Goâ„¢ Free Office Suite'], ['Instachat ðŸ˜œ']]\n",
    "test_list_appstore = [['','','Instagram'], ['','','çˆ±å¥‡è‰ºPPS -ã€Šæ¬¢ä¹é¢‚2ã€‹ç”µè§†å‰§çƒ­æ’­'], ['','','Docs To Goâ„¢ Free Office Suite'], ['','','Instachat ðŸ˜œ']]\n",
    "\n",
    "test_list_android = remove_non_english(test_list_android, 0)\n",
    "test_list_appstore = remove_non_english(test_list_appstore, 2)\n",
    "print(test_list_android, '\\n', test_list_appstore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c1d81",
   "metadata": {},
   "source": [
    "As we can see, our cleaning functions got rid of an app named **çˆ±å¥‡è‰ºPPS -ã€Šæ¬¢ä¹é¢‚2ã€‹ç”µè§†å‰§çƒ­æ’­**, but left **Instachat ðŸ˜œ** and **Docs To Goâ„¢ Free Office Suite**, both of which also contain non-english characters. Now that we've confirmed that everything works, we can remove non-english apps from our data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73dcf2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining number of android apps:  9615\n",
      "Remaining number of ios apps:  6184\n"
     ]
    }
   ],
   "source": [
    "googleplay_dataset = remove_non_english(googleplay_dataset, 0)\n",
    "appstore_dataset = remove_non_english(appstore_dataset, 2)\n",
    "print('Remaining number of android apps: ', len(googleplay_dataset))\n",
    "print('Remaining number of ios apps: ', len(appstore_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733d73ed",
   "metadata": {},
   "source": [
    "The last thing we have to do to clean our data is removing any non-free apps. We can see that in both data sets prices are strings and the price of each free app is a string `'0'`. It allows us to iterate through our data sets and to assign free apps(apps containing `'0'` in price olumn) to a new list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "620d602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_free(dataset, price_index):\n",
    "    clean_data = []\n",
    "    clean_data.append(dataset[0])\n",
    "    for row in dataset[1:]:\n",
    "        if row[price_index] == '0':\n",
    "            clean_data.append(row)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75df0d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of free android apps:  8863\n",
      "Number of free ios apps:  3223\n"
     ]
    }
   ],
   "source": [
    "googleplay_dataset = remove_non_free(googleplay_dataset, 7)\n",
    "appstore_dataset = remove_non_free(appstore_dataset, 5)\n",
    "print('Number of free android apps: ', len(googleplay_dataset))\n",
    "print('Number of free ios apps: ', len(appstore_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580dd9f",
   "metadata": {},
   "source": [
    "As we can see, after the final cleaning we're left with 8862 Android apps and 3222 iOS apps. We can now move on to finding the most suitable app profile for our analysis. Our app is going to be free, so we need to determine the kinds of apps that are likely to attract the most users. \n",
    "\n",
    "Our validation strategy for an app idea is made of three steps:\n",
    "- We create a minimal version of our app and add it to Google Play\n",
    "- If the app gets a good response we develop it futher\n",
    "- If the app is profitable after six months, we build an iOS version and add it to App Store\n",
    "\n",
    "Since our goal is to find an app that will perform well on Google Play and App Store, we need to find app profiles that are successful on both markets. We can start our analysis by finding the most popular genres on both platforms. To do that, we will write a function that creates an empty dictionary, fills it with genres listed in our data sets and if the app's genre already exists in our dictionary, it will increase our genre's frequency by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158784ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
